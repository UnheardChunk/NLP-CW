{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which pre-trained model to use.\n",
    "checkpoint = \"microsoft/deberta-v3-small\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_path = \"./venv/nltk_data\"\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"wordnet\", download_dir=nltk_path)\n",
    "nltk.download(\"stopwords\", download_dir=nltk_path)\n",
    "nltk.download(\"omw-1.4\", download_dir=nltk_path)\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\", download_dir=nltk_path)\n",
    "nltk.download('punkt_tab',download_dir=nltk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "from models.data_augmentation import synonym_replacement\n",
    "from models.preprocessing import preprocess\n",
    "from models.data_sampling import class_weighting \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether you are running on CPU or GPU.\n",
    "# If the output here says \"cuda\" then it's running on GPU. Otherwise it's probably CPU.\n",
    "# In order to run your code in Colab on the GPU, go to Edit -> Notebook settings -> Hardware accelerator and set it to \"GPU\".\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "official_dev_dataset_path =\"../dataset/dpm_pcl_test.csv\"\n",
    "official_dev_dataset = pd.read_csv(official_dev_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "official_dev_dataset[\"label\"] = official_dev_dataset[\"label\"].apply(lambda x: 0 if (x == 0 or x == 1) else 1)\n",
    "\n",
    "official_dev_dataset.loc[official_dev_dataset[\"label\"].isna(), \"text\"] = \"\"\n",
    "\n",
    "official_dev_dataset = official_dev_dataset.drop(['par_id', 'art_id', 'keyword', 'country', 'orig_label'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, model_max_length=512)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess datasets\n",
    "preprocessed_official_dev_dataset =  preprocess(official_dev_dataset, remove_stopwords=False)\n",
    "\n",
    "\n",
    "# Convert into Dataset objects\n",
    "raw_preprocessed_official_dev_dataset = Dataset.from_pandas(preprocessed_official_dev_dataset[\"text\", \"label\"])\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_preprocessed_official_dev_dataset = raw_preprocessed_official_dev_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model's state\n",
    "def load_model(model, filename):\n",
    "    model_dir = \"./models/saved_models\"\n",
    "    saved_model = torch.load(model_dir + filename)\n",
    "    model.load_state_dict(saved_model[\"model_state_dict\"])\n",
    "\n",
    "def get_and_save_predictions(model, dataloader, filename):\n",
    "    print(\"Getting predictions...\")\n",
    "    \n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            predictions.extend(logits.argmax(dim=-1).cpu().numpy())\n",
    "    \n",
    "    print(\"Saving predictions...\")\n",
    "    \n",
    "    # Save predictions as a .txt file with one prediction per line\n",
    "    with open(filename, \"w\") as f:\n",
    "        for prediction in predictions:\n",
    "            f.write(f\"{prediction}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# Load the model's state\n",
    "load_model(model, \"deberta_with_preprocessing_synonym_replacement_and_class_weighting.pth\")\n",
    "\n",
    "# Load the datasets\n",
    "official_dev_dataloader = DataLoader(\n",
    "    tokenized_preprocessed_official_dev_dataset, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "get_and_save_predictions(model, official_dev_dataloader, \"dev.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
