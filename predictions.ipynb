{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which pre-trained model to use.\n",
    "checkpoint = \"microsoft/deberta-v3-small\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to ./venv/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to ./venv/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to ./venv/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     ./venv/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to ./venv/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_path = \"./venv/nltk_data\"\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"wordnet\", download_dir=nltk_path)\n",
    "nltk.download(\"stopwords\", download_dir=nltk_path)\n",
    "nltk.download(\"omw-1.4\", download_dir=nltk_path)\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\", download_dir=nltk_path)\n",
    "nltk.download('punkt_tab',download_dir=nltk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "from models.preprocessing import preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Checking whether you are running on CPU or GPU.\n",
    "# If the output here says \"cuda\" then it's running on GPU. Otherwise it's probably CPU.\n",
    "# In order to run your code in Colab on the GPU, go to Edit -> Notebook settings -> Hardware accelerator and set it to \"GPU\".\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "official_dev_dataset_path = \"./dataset/dpm_pcl_test.csv\"\n",
    "test_dataset_path = \"./dataset/original_datasets/task4_test.tsv\"\n",
    "\n",
    "official_dev_dataset = pd.read_csv(official_dev_dataset_path)\n",
    "test_dataset = pd.read_csv(test_dataset_path, sep=\"\\t\", names=['par_id', 'art_id', 'keyword', 'country', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>His present \" chambers \" may be quite humble ,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Krueger recently harnessed that creativity to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10:41am - Parents of children who died must ge...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When some people feel causing problem for some...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We are alarmed to learn of your recently circu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  His present \" chambers \" may be quite humble ,...      1\n",
       "1  Krueger recently harnessed that creativity to ...      1\n",
       "2  10:41am - Parents of children who died must ge...      1\n",
       "3  When some people feel causing problem for some...      1\n",
       "4  We are alarmed to learn of your recently circu...      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "official_dev_dataset[\"label\"] = official_dev_dataset[\"orig_label\"].apply(lambda x: 0 if (x == 0 or x == 1) else 1)\n",
    "\n",
    "official_dev_dataset.loc[official_dev_dataset[\"text\"].isna(), \"text\"] = \"\"\n",
    "test_dataset.loc[test_dataset[\"text\"].isna(), \"text\"] = \"\"\n",
    "\n",
    "official_dev_dataset = official_dev_dataset.drop(['par_id', 'art_id', 'keyword', 'country', 'orig_label'], axis=1)\n",
    "test_dataset = test_dataset.drop(['par_id', 'art_id', 'keyword', 'country'], axis=1)\n",
    "\n",
    "official_dev_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/gd221/NLP-CW/venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, model_max_length=512)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c74365f7602a413ea99622e96d5a8179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2094 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a38b4a004394860932e4ca3be73bbfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preprocess datasets\n",
    "preprocessed_official_dev_dataset =  preprocess.preprocess_dataset(official_dev_dataset, remove_stopwords=False)\n",
    "preprocessed_test_dataset = preprocess.preprocess_dataset(test_dataset, remove_stopwords=False)\n",
    "\n",
    "# Convert into Dataset objects\n",
    "raw_preprocessed_official_dev_dataset = Dataset.from_pandas(preprocessed_official_dev_dataset[[\"text\", \"label\"]])\n",
    "raw_preprocessed_test_dataset = Dataset.from_pandas(preprocessed_test_dataset[[\"text\"]])\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_preprocessed_official_dev_dataset = raw_preprocessed_official_dev_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_preprocessed_test_dataset = raw_preprocessed_test_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model's state\n",
    "def load_model(model, filename):\n",
    "    model_dir = \"./models/saved_models/\"\n",
    "    saved_model = torch.load(model_dir + filename)\n",
    "    model.load_state_dict(saved_model[\"model_state_dict\"])\n",
    "\n",
    "def get_and_save_predictions(model, dataloader, filename):\n",
    "    print(\"Getting predictions...\")\n",
    "    \n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            predictions.extend(logits.argmax(dim=-1).cpu().numpy())\n",
    "    \n",
    "    print(\"Saving predictions...\")\n",
    "    \n",
    "    # Save predictions as a .txt file with one prediction per line\n",
    "    with open(filename, \"w\") as f:\n",
    "        for prediction in predictions:\n",
    "            f.write(f\"{prediction}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions...\n",
      "Saving predictions...\n",
      "Getting predictions...\n",
      "Saving predictions...\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# Load the model's state\n",
    "load_model(model, \"deberta_with_preprocessing_synonym_replacement_and_class_weighting.pth\")\n",
    "\n",
    "# Load the datasets\n",
    "official_dev_dataloader = DataLoader(\n",
    "    tokenized_preprocessed_official_dev_dataset, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_preprocessed_test_dataset, batch_size=8, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "# Make dev.txt and test.txt predictions files\n",
    "get_and_save_predictions(model, official_dev_dataloader, \"dev.txt\")\n",
    "get_and_save_predictions(model, test_dataloader, \"test.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
